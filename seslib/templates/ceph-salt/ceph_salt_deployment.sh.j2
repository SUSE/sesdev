
set -ex

{% if ceph_salt_git_repo and ceph_salt_git_branch %}
# install ceph-salt
cd /root
git clone {{ ceph_salt_git_repo }}
cd ceph-salt
zypper --non-interactive install autoconf gcc python3-devel python3-pip python3-curses

{% if ceph_salt_fetch_github_pr_heads %}
# fetch the available PRs (HEAD) from github. With that, "ceph_salt_git_branch" can be something like "origin/pr/127" to checkout a github PR
git fetch origin "+refs/pull/*/head:refs/remotes/origin/pr/*"
{% endif %}
{% if ceph_salt_fetch_github_pr_merges %}
# fetch the available PRs (MERGE) from github. With that, "ceph_salt_git_branch" can be something like "origin/pr-merged/127" to checkout a github PR
git fetch origin "+refs/pull/*/merge:refs/remotes/origin/pr-merged/*"
{% endif %}

git checkout {{ ceph_salt_git_branch }}

pip install --prefix /usr .
# install ceph-salt-formula
cp -r ceph-salt-formula/salt/* /srv/salt/
chown -R salt:salt /srv
{% else %}
# ceph-salt-formula is installed automatically as a dependency of ceph-salt
zypper --non-interactive install ceph-salt
{% endif %}

systemctl restart salt-master
{% include "wait_for_minions.sh.j2" %}

{% if use_salt %}
salt '*' saltutil.pillar_refresh
salt '*' saltutil.sync_all
sleep 2
{% endif %}

{% if stop_before_ceph_salt_config %}
exit 0
{% endif %}

echo "PATH is $PATH"
type ceph-salt

{% for node in nodes %}
{% if node.has_roles() and not node.has_exclusive_role('client') %}
ceph-salt config /ceph_cluster/minions add {{ node.fqdn }}
ceph-salt config /ceph_cluster/roles/cephadm add {{ node.fqdn }}
ceph-salt config /ceph_cluster/roles/admin add {{ node.fqdn }}
{% endif %}
{% if node.has_role('bootstrap') %}
ceph-salt config /ceph_cluster/roles/bootstrap set {{ node.fqdn }}
{% endif %}
{% endfor %}

ceph-salt config /system_update/packages disable
ceph-salt config /system_update/reboot disable
ceph-salt config /ssh/ generate
{% if image_path %}
ceph-salt config /containers/images/ceph set {{ image_path }}
{% endif %}
ceph-salt config /time_server/server_hostname set {{ master.fqdn }}
{% set external_timeserver = "pool.ntp.org" %}
ceph-salt config /time_server/external_servers add {{ external_timeserver }}

{% if storage_nodes < 3 %}
ceph-salt config /cephadm_bootstrap/ceph_conf add global
ceph-salt config /cephadm_bootstrap/ceph_conf/global set "osd crush chooseleaf type" 0
{% endif %}

ceph-salt config /cephadm_bootstrap/dashboard/username set admin
ceph-salt config /cephadm_bootstrap/dashboard/password set admin
ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable

ceph-salt config /cephadm_bootstrap/monitoring_stack
{%- if bootstrap_node_monitoring_stack %}
 enable
{%- else %}
 disable
{%- endif %}

ceph-salt config ls
ceph-salt export --pretty
ceph-salt status

zypper repos -upEP
zypper info cephadm | grep -E '(^Repo|^Version)'
ceph-salt --version

{% if stop_before_ceph_salt_apply %}
exit 0
{% endif %}

{% if use_salt %}
salt -G 'ceph-salt:member' state.apply ceph-salt
{% else %}
stdbuf -o0 ceph-salt -ldebug apply --non-interactive
{% endif %}

{% if stop_before_ceph_orch_apply %}
exit 0
{% endif %}

{% if mon_nodes > 1 %}
ceph orch apply mon "{{ mon_node_list }}"
{% endif %} {# mon_nodes > 1 #}

{% if mgr_nodes > 1 %}
ceph orch apply mgr "{{ mgr_node_list }}"
{% endif %} {# mgr_nodes > 1 #}

{% if storage_nodes > 0 %}
{% set service_spec_osd = "service_spec_osd.yml" %}
cat > {{ service_spec_osd }} << EOF
service_type: osd
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('storage') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
service_id: generic_osd_deployment
data_devices:
    all: true
{% if encrypted_osds %}
encrypted: true
{% endif %}
{% if filestore_osds %}
objectstore: filestore
{% endif %}
EOF
cat {{ service_spec_osd }}

ceph orch device ls --refresh
ceph orch apply osd -i {{ service_spec_osd }}
{% endif %} {# storage_nodes > 0 #}

{% if mds_nodes > 0 %}
ceph fs volume create myfs "{{ mds_node_list }}"
{% endif %}

{% if rgw_nodes > 0 %}
radosgw-admin realm create --rgw-realm=default --default
radosgw-admin zonegroup create --rgw-zonegroup=default --master --default
radosgw-admin zone create --rgw-zonegroup=default --rgw-zone=default --master --default
ceph orch apply rgw default default --placement="{{ rgw_node_list }}"
{% endif %}

{% if nfs_nodes > 0 %}
ceph osd pool create rbd
ceph osd pool application enable rbd nfs
ceph orch apply nfs default rbd nfs --placement="{{ nfs_node_list }}"
{% endif %}

{% include "qa_test.sh.j2" %}
