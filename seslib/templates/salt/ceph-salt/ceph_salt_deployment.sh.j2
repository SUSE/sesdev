
set -ex

REASONABLE_TIMEOUT_IN_SECONDS="1800"

{% if ceph_salt_git_repo and ceph_salt_git_branch %}
# install ceph-salt
cd /root
git clone {{ ceph_salt_git_repo }}
cd ceph-salt
zypper --non-interactive install autoconf gcc python3-devel python3-pip python3-curses

{% if ceph_salt_fetch_github_pr_heads %}
# fetch the available PRs (HEAD) from github. With that, "ceph_salt_git_branch" can be something like "origin/pr/127" to checkout a github PR
git fetch origin "+refs/pull/*/head:refs/remotes/origin/pr/*"
{% endif %}
{% if ceph_salt_fetch_github_pr_merges %}
# fetch the available PRs (MERGE) from github. With that, "ceph_salt_git_branch" can be something like "origin/pr-merged/127" to checkout a github PR
git fetch origin "+refs/pull/*/merge:refs/remotes/origin/pr-merged/*"
{% endif %}

git checkout {{ ceph_salt_git_branch }}

pip install --prefix /usr .
# install ceph-salt-formula
cp -r ceph-salt-formula/salt/* /srv/salt/
chown -R salt:salt /srv
{% else %}
# ceph-salt-formula is installed automatically as a dependency of ceph-salt
zypper --non-interactive install ceph-salt
{% endif %}

systemctl restart salt-master
{% include "salt/wait_for_minions.j2" %}

{% if use_salt %}
salt '*' saltutil.pillar_refresh
salt '*' saltutil.sync_all
sleep 2
{% endif %}

{% if stop_before_ceph_salt_config %}
set +x
echo "Stopping the deployment now because --stop-before-ceph-salt-config option was given."
set -x
exit 0
{% endif %} {# stop_before_ceph_salt_config #}

echo "PATH is $PATH"
type ceph-salt
ceph-salt --version

{% for node in nodes %}
{% if not node.has_exclusive_role('client') %}
ceph-salt config /ceph_cluster/minions add {{ node.fqdn }}
ceph-salt config /ceph_cluster/roles/cephadm add {{ node.fqdn }}
{% endif %}
{% if node.has_role('admin') %}
ceph-salt config /ceph_cluster/roles/admin add {{ node.fqdn }}
{% endif %}
{% endfor %}
ceph-salt config /ceph_cluster/roles/bootstrap set {{ cephadm_bootstrap_node.fqdn }}

ceph-salt config /ssh/ generate
ceph-salt config /time_server/server_hostname set {{ master.fqdn }}
{% set external_timeserver = "pool.ntp.org" %}
ceph-salt config /time_server/external_servers add {{ external_timeserver }}

{% if image_path %}
ceph-salt config /cephadm_bootstrap/ceph_image_path set {{ image_path }}
{% endif %}

{% if storage_nodes < 3 %}
ceph-salt config /cephadm_bootstrap/ceph_conf add global
ceph-salt config /cephadm_bootstrap/ceph_conf/global set "osd crush chooseleaf type" 0
{% endif %}

ceph-salt config /cephadm_bootstrap/dashboard/username set admin
ceph-salt config /cephadm_bootstrap/dashboard/password set admin
ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable

ceph-salt config ls
ceph-salt export --pretty
ceph-salt status

zypper repos --details
zypper info cephadm | grep -E '(^Repo|^Version)'
ceph-salt --version

{% if stop_before_ceph_salt_apply %}
set +x
echo "Stopping the deployment now because --stop-before-ceph-salt-apply option was given."
set -x
exit 0
{% endif %} {# stop_before_ceph_salt_apply #}

{% if use_salt %}
salt -G 'ceph-salt:member' state.apply ceph-salt
echo "\"salt state.apply\" exit code: $?"
{% else %}
stdbuf -o0 ceph-salt -ldebug apply --non-interactive
echo "\"ceph-salt apply\" exit code: $?"
{% endif %}

{% if stop_before_ceph_orch_apply %}
set +x
echo "Stopping the deployment now because --stop-before-ceph-orch-apply option was given."
set -x
exit 0
{% endif %} {# stop_before_ceph_orch_apply #}

# Wait for the bootstrap MON+MGR to appear

function number_of_foos_in_bootstrap_cephadm_ls {
    local foo="$1"
    set -x
    ssh {{ cephadm_bootstrap_node.fqdn }} cephadm ls | jq "[ .[].name | select(startswith(\"$foo\")) ] | length"
    set +x
}

set +x
timeout_seconds="$REASONABLE_TIMEOUT_IN_SECONDS"
remaining_seconds="$timeout_seconds"
interval_seconds="30"
while true ; do
    ACTUAL_NUMBER_OF_MONS="$(number_of_foos_in_bootstrap_cephadm_ls mon)"
    ACTUAL_NUMBER_OF_MGRS="$(number_of_foos_in_bootstrap_cephadm_ls mgr)"
    echo "MONs in cluster (actual/expected): $ACTUAL_NUMBER_OF_MONS/1 (${remaining_seconds} seconds to timeout)"
    echo "MGRs in cluster (actual/expected): $ACTUAL_NUMBER_OF_MGRS/1 (${remaining_seconds} seconds to timeout)"
    remaining_seconds="$(( remaining_seconds - interval_seconds ))"
    [ "$ACTUAL_NUMBER_OF_MONS" -ge "1" ] && [ "$ACTUAL_NUMBER_OF_MGRS" -ge "1" ] && break
    if [ "$remaining_seconds" -le "0" ] ; then
        set -x
        ceph status
        set +x
        echo "It seems unlikely that the bootstrap MON and MGR will ever appear. Bailing out!"
        false
    fi  
    echo "..."
    sleep "$interval_seconds"
done
if [ "$ACTUAL_NUMBER_OF_MONS" -gt "1" ] ; then
    echo
    echo
    echo "WARNING: \"cephadm bootstrap\" deployed more MONs than expected!"
    echo
    echo
fi
if [ "$ACTUAL_NUMBER_OF_MGRS" -gt "1" ] ; then
    echo
    echo
    echo "WARNING: \"cephadm bootstrap\" deployed more MGRs than expected!"
    echo
    echo
fi
set -x

ceph status
echo "\"ceph status\" exit code: $?"

{% set service_spec_core = "/root/service_spec_core.yml" %}
rm -f {{ service_spec_core }}
touch {{ service_spec_core }}

{% if mon_nodes > 1 or mgr_nodes > 1 or storage_nodes > 0 %}

{% if mon_nodes > 1 %}
cat >> {{ service_spec_core }} << 'EOF'
---
service_type: mon
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('mon') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# mon_nodes > 1 #}

{% if mgr_nodes > 1 %}
cat >> {{ service_spec_core }} << 'EOF'
---
service_type: mgr
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('mgr') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# mgr_nodes > 1 #}

{% if storage_nodes > 0 %}
cat >> {{ service_spec_core }} << 'EOF'
---
service_type: osd
service_id: sesdev_osd_deployment
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('storage') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
data_devices:
    all: true
{% if encrypted_osds %}
encrypted: true
{% endif %}
{% if filestore_osds %}
objectstore: filestore
{% endif %}
EOF
{% endif %} {# storage_nodes > 0 #}

set +x
echo "Allowing one-minute grace period to expire before initiating \"ceph orch apply\"."
sleep 60
set -x
cat {{ service_spec_core }}
ceph orch apply -i {{ service_spec_core }} --dry-run
ceph orch apply -i {{ service_spec_core }}

{% endif %} {# mon_nodes > 1 or mgr_nodes > 1 or storage_nodes > 0 #}

{% if total_osds > 0 %}

# Wait for OSDs to appear
function number_of_osds_in_ceph_osd_tree {
    ceph osd tree -f json-pretty | jq '[.nodes[] | select(.type == "osd")] | length'
}

EXPECTED_NUMBER_OF_OSDS="{{ total_osds }}"

set +x
timeout_seconds="$((REASONABLE_TIMEOUT_IN_SECONDS * 2))"
remaining_seconds="$timeout_seconds"
interval_seconds="30"
while true ; do
    ACTUAL_NUMBER_OF_OSDS="$(number_of_osds_in_ceph_osd_tree)"
    echo "OSDs in cluster (actual/expected): $ACTUAL_NUMBER_OF_OSDS/$EXPECTED_NUMBER_OF_OSDS (${remaining_seconds} seconds to timeout)"
    remaining_seconds="$(( remaining_seconds - interval_seconds ))"
    [ "$ACTUAL_NUMBER_OF_OSDS" = "$EXPECTED_NUMBER_OF_OSDS" ] && break
    if [ "$remaining_seconds" -le "0" ] ; then
        set -x
        ceph status
        set +x
        echo "It seems unlikely that this cluster will ever reach the expected number of OSDs. Bailing out!"
        false
    fi  
    sleep "$interval_seconds"
done
set -x

{% endif %} {# if total_osds > 0 #}

{% if storage_nodes > 0 %}

{% if mds_nodes > 0 or rgw_nodes > 0 or nfs_nodes > 0 %}

NFS_NODES_COMMA_SEPARATED_LIST=""
{% for node in nodes %}
{% if node.has_role('mds') %}
{% endif %}
{% if node.has_role('nfs') %}
NFS_NODES_COMMA_SEPARATED_LIST+="{{ node.name }},"
{% endif %}
{% endfor %}
NFS_NODES_COMMA_SEPARATED_LIST="${NFS_NODES_COMMA_SEPARATED_LIST%,*}"

{% endif %} {# mds_nodes > 0 or rgw_nodes > 0 or nfs_nodes > 0 #}

{% if mds_nodes > 0 %}

{% set service_spec_mds = "/root/service_spec_mds.yml" %}
{% set fs_volume = "sesdev_fs" %}
cat >> {{ service_spec_mds }} << 'EOF'
---
service_type: mds
service_id: {{ fs_volume }}
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('mds') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
ceph orch apply -i {{ service_spec_mds }}

# wait for the MDSs to appear

function number_of_foos_ceph_orch_ls {
    local service_type="$1"
    local ceph_orch_ls
    local running
    ceph_orch_ls="$(ceph orch ls --service-type "$service_type" --format json)"
    if echo "$ceph_orch_ls" | jq -r >/dev/null 2>&1 ; then
        running="$(echo "$ceph_orch_ls" | jq -r '.[] | .status.running')"
        echo "$running"
    else
        echo "0"
    fi
}

EXPECTED_NUMBER_OF_MDSS="{{ mds_nodes }}"

set +x
timeout_seconds="$REASONABLE_TIMEOUT_IN_SECONDS"
remaining_seconds="$timeout_seconds"
interval_seconds="30"
while true ; do
    ACTUAL_NUMBER_OF_MDSS="$(number_of_foos_ceph_orch_ls mds)"
    echo "MDSs in cluster (actual/expected): $ACTUAL_NUMBER_OF_MDSS/$EXPECTED_NUMBER_OF_MDSS (${remaining_seconds} seconds to timeout)"
    remaining_seconds="$(( remaining_seconds - interval_seconds ))"
    [ "$ACTUAL_NUMBER_OF_MDSS" = "$EXPECTED_NUMBER_OF_MDSS" ] && break
    if [ "$remaining_seconds" -le "0" ] ; then
        set -x
        ceph status
        set +x
        echo "It seems unlikely that this cluster will ever reach the expected number of MDSs. Bailing out!"
        false
    fi  
    sleep "$interval_seconds"
done
set -x

# create the volume
ceph fs volume create {{ fs_volume }}
sleep 10
ceph fs status
sleep 5
ceph fs status --format json-pretty

{% if nfs_nodes > 0 %}
{% set nfs_cluster = "sesdev_nfs" %}
ceph nfs cluster create cephfs {{ nfs_cluster }} "$NFS_NODES_COMMA_SEPARATED_LIST"
sleep 10
ceph nfs cluster ls
{% endif %} {# nfs_nodes > 0 #}

{% endif %} {# mds_nodes > 0 #}

{% set deploy_monitoring = False %}
{% if prometheus_nodes > 0 or grafana_nodes > 0 or alertmanager_nodes > 0 or node_exporter_nodes > 0 %}
{% set deploy_monitoring = True %}
# manually enabling prometheus module will no longer be required once
# https://tracker.ceph.com/issues/46606 is fixed
ceph mgr module enable prometheus
{% endif %} {# prometheus_nodes > 0 or grafana_nodes > 0 or alertmanager_nodes > 0 or node_exporter_nodes > 0 #}

{% if rgw_nodes > 0 or igw_nodes > 0 or deploy_monitoring %}

{% set service_spec_gw = "/root/service_spec_gw.yml" %}
rm -f {{ service_spec_gw }}
touch {{ service_spec_gw }}

{% if rgw_nodes > 0 %}
radosgw-admin realm create --rgw-realm=default --default
radosgw-admin zonegroup create --rgw-zonegroup=default --master --default
radosgw-admin zone create --rgw-zonegroup=default --rgw-zone=default --master --default
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: rgw
service_id: default.default
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('rgw') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# rgw_nodes > 0 #}

# pre-create pool for IGW if needed
{% if igw_nodes > 0 %}
ceph osd pool create rbd
{% endif %} {# igw_nodes > 0 #}

{% if igw_nodes > 0 %}
ceph osd pool application enable rbd rbd --yes-i-really-mean-it
{% set trusted_ip_list = [] %}
{% for node in nodes %}
{% if node.has_role('mgr') or node.has_role('igw') %}
{% if trusted_ip_list.append(node.public_address) %}{% endif %}
{% endif %}
{% endfor %}
cat >> {{ service_spec_gw }} << EOF
---
service_type: iscsi
service_id: iscsi_service
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('igw') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
spec:
    pool: rbd
    trusted_ip_list: {{ trusted_ip_list | join(',') }}
    api_port: 5000
    api_user: admin1
    api_password: admin2
    api_secure: False
EOF
{% endif %} {# igw_nodes > 0 #}

{% if prometheus_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: prometheus
service_id: sesdev_monitoring_prometheus
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('prometheus') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# prometheus_nodes > 0 #}

{% if grafana_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: grafana
service_id: sesdev_monitoring_grafana
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('grafana') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# grafana_nodes > 0 #}

{% if alertmanager_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: alertmanager
service_id: sesdev_monitoring_alertmanager
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('alertmanager') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# alertmanager_nodes > 0 #}

{% if node_exporter_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: node-exporter
service_id: sesdev_monitoring_node_exporter
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('node-exporter') %}
        - '{{ node.name }}'
{% endif %}
{% endfor %}
EOF
{% endif %} {# node_exporter_nodes > 0 #}

cat {{ service_spec_gw }}
ceph orch apply -i {{ service_spec_gw }}

{% endif %} {# rgw_nodes > 0 or igw_nodes > 0 or deploy_monitoring #}

{% endif %} {# storage_nodes > 0 #}

{% include "salt/qa_test.j2" %}
