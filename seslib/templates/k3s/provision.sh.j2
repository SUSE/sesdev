set -ex

# curl is needed to download the k3s and helm installers
# the helm installer needs openssl to do checksum verification
zypper --non-interactive install curl openssl

# need this to pick up kubectl, helm etc. which are installed to /usr/local/bin
export PATH=$PATH:/usr/local/bin

{% if k3s_deploy_longhorn %}
zypper --non-interactive install nfs-client open-iscsi e2fsprogs lvm2
systemctl enable iscsid
systemctl start iscsid
{% endif %} {# k3s_deploy_longhorn #}

{% if k3s_version %}
export INSTALL_K3S_VERSION={{ k3s_version }}
{% endif %}

{% if node == master %}

# Lifted from seslib/templates/caasp/master.sh.j2
function wait_for_master_ready {
    set +ex
    echo "Waiting for master to be ready"
    timeout_seconds="900"
    remaining_seconds="$timeout_seconds"
    interval_seconds="10"
    while true ; do
        set -x
        ACTUAL_NUMBER_OF_MASTERS="$(kubectl get nodes 2>/dev/null | egrep -c "master\s+Ready")"
        set +x
        echo "masters in cluster (actual/expected): $ACTUAL_NUMBER_OF_MASTERS/1 (${remaining_seconds} seconds to timeout)"
        remaining_seconds="$(( remaining_seconds - interval_seconds ))"
        [ "$ACTUAL_NUMBER_OF_MASTERS" = "1" ] && break
        if [ "$remaining_seconds" -le "0" ] ; then
            echo "It seems unlikely that a master will ever appear. Bailing out!"
            set -e
            false
        fi
        sleep "$interval_seconds"
    done
    set -ex
}

# Also lifted from seslib/templates/caasp/master.sh.j2, except for the
# ACTUAL_NUMBER_O_F_WORKERS= line, which checked for "worker[0-9]+\s+Ready",
# but that won't do because the nodes are named "node[0-9]", not "worker[0-9]"
function wait_for_workers_ready {
    set +ex
    echo "Waiting for {{ worker_nodes }} workers to be ready"
    timeout_seconds="900"
    remaining_seconds="$timeout_seconds"
    interval_seconds="10"
    while true ; do
        set -x
        ACTUAL_NUMBER_OF_WORKERS="$(kubectl get nodes 2>/dev/null | egrep -c "node[0-9]+\s+Ready")"
        set +x
        echo "workers in cluster (actual/expected): $ACTUAL_NUMBER_OF_WORKERS/{{ worker_nodes }} (${remaining_seconds} seconds to timeout)"
        remaining_seconds="$(( remaining_seconds - interval_seconds ))"
        [ "$ACTUAL_NUMBER_OF_WORKERS" = "{{ worker_nodes }}" ] && break
        if [ "$remaining_seconds" -le "0" ] ; then
            echo "It seems unlikely that {{ worker_nodes }} workers will ever appear. Bailing out!"
            set -e
            false
        fi
        sleep "$interval_seconds"
    done
    set -ex
}

curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -

# Unfortunately we can't use `kubectl wait --for=condition=Ready node/{{ master.name }} --timeout 15m`
# because this runs before the master node even exists, and so we get
# 'Error from server (NotFound): nodes "master" not found'
wait_for_master_ready

wait_for_workers_ready

# Helm seems generally useful, let's install it
curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version

# This sets KUBECONFIG for everyone to the global k3s config.
# Without this, helm will try to talk to http://localhost:8080/version
# by default, which of course won't work.
echo "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" >> /etc/profile.local

# The earlier addition of KUBECONFIG to /etc/profile.local won't help
# us in this current session, so also need to set it explicitly here.
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

{% if k3s_deploy_ses %}

mkdir -p ~/cluster
cd ~/cluster
helm pull oci://registry.suse.com/ses/7.1/charts/rook-ceph
# Currently rook-ceph-1.10.1.tgz
# TODO: can we ask helm for the name of the file?  Or just assume it's
# the only file currently in the directory?
tar -xzf rook-ceph-*.tgz

kubectl create namespace rook-ceph

helm install -n rook-ceph rook-ceph ./rook-ceph/

echo "Waiting for the rook operator"
kubectl wait --timeout=5m --namespace rook-ceph --for=condition=Ready pod  -l "app=rook-ceph-operator"

echo "Let rook take all nodes that aren't the master"
kubectl label node -l 'node-role.kubernetes.io/master!=true' node-role.rook-ceph/cluster=any

echo "Creating ceph cluster..."
kubectl create -f rook-ceph/examples/cluster.yaml

# The above will take some time (maybe 20 minutes in my testing),
# so let's not wait for it :-)

echo "Creating toolbox container..."
kubectl create -f rook-ceph/examples/toolbox.yaml

{% endif %} {# k3s_deploy_ses #}

{% if k3s_deploy_longhorn %}

# Make longhorn create default disks on all nodes that aren't the master
kubectl label node -l 'node-role.kubernetes.io/master!=true' node.longhorn.io/create-default-disk=true

helm repo add longhorn https://charts.longhorn.io
helm repo update
helm install longhorn longhorn/longhorn \
    --namespace longhorn-system \
    --create-namespace {{ "--version {}".format(longhorn_version) if longhorn_version }} \
    --set defaultSettings.createDefaultDiskLabeledNodes=true

# On k3s this just gives a traefik ingress for the UI with no authentication
echo "
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: longhorn-ingress
  namespace: longhorn-system
spec:
  rules:
  - http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: longhorn-frontend
            port:
              number: 80
" | kubectl -n longhorn-system create -f -

kubectl -n longhorn-system get ingress

{% endif %} {# k3s_deploy_longhorn #}

{% else %} {# node == master #}

function get_k3s_token {
    set +ex
    echo "Waiting for K3S token from master"
    timeout_seconds="900"
    remaining_seconds="$timeout_seconds"
    interval_seconds="30"
    while true ; do
        remaining_seconds="$(( remaining_seconds - interval_seconds ))"
        if scp master:/var/lib/rancher/k3s/server/node-token /tmp/k3s_token 2>/dev/null ; then
            # Got the node token, probably (when exactly does that token get created
            # on the master? could we conceivably have a weird race where we get an
            # empty token file here?)
            break
        fi
        if [ "$remaining_seconds" -le "0" ] ; then
            echo "It seems unlikely that a master will ever appear. Bailing out!"
            set -e
            false
        fi
        echo "waiting for k3s token (${remaining_seconds} seconds to timeout)"
        sleep "$interval_seconds"
    done
    set -ex
}

get_k3s_token
export K3S_TOKEN=$(cat /tmp/k3s_token)
rm /tmp/k3s_token

curl -sfL https://get.k3s.io | K3S_URL=https://{{ master.fqdn }}:6443 sh -

{% if k3s_deploy_longhorn %}
if [ ! -b /dev/vdb ]; then
    echo "ERROR: Longhorn deployments require at least one additional disk"
    false
fi

# This won't handle more than 25 disks.  Tim is pretty sure that's fine.
vgcreate longhorn /dev/vd[b-z]
lvcreate --name longhorn-data -l 100%VG longhorn
mkfs.ext4 /dev/longhorn/longhorn-data
mkdir /var/lib/longhorn
echo "/dev/longhorn/longhorn-data /var/lib/longhorn ext4 defaults 0 2" >> /etc/fstab
mount /var/lib/longhorn
{% endif %} {# k3s_deploy_longhorn #}

{% endif %}
