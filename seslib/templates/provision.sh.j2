set -e

DEPLOYMENT_SCRIPT="$0"

function err_report {
    local hn
    set +x
    if hostname >/dev/null 2>&1 ; then
        hn="$(hostname)"
    else
        hn="(cannot determine: hostname binary appears to be missing)"
    fi
    local line_number="$1"
    echo "Error in provisioner script trapped!"
    echo "=> hostname: $hn"
    echo "=> script: $DEPLOYMENT_SCRIPT"
    echo "=> line number: $line_number"
    echo "Bailing out!"
    exit 1
}

# We run hostname in err_report. Blindly try to make sure it's installed.
if hostname >/dev/null 2>&1 ; then
    echo "hostname binary is present. Good."
else
    if zypper >/dev/null 2>&1 ; then
        echo "hostname binary is not present. Attempting to install it using \"zypper\"..."
        set -x
        zypper --non-interactive install hostname || true
        set +x
    fi
fi

set -x

# display error report when this provisioner script fails
trap 'err_report $LINENO' ERR

# do not limit coredump size
ulimit -c unlimited

ls -lR /home/vagrant

# populate /etc/hosts
{% for _node in nodes %}
{% if _node.public_address %}
echo "{{ _node.public_address }} {{ _node.fqdn }} {{ _node.name }}" >> /etc/hosts
{% endif %}
{% endfor %}

# Vagrant sometimes adds a bogus 127.0.0.1 line at the beginning of /etc/hosts:
# If it's there, delete it!
echo "State of /etc/hosts loopback assignments (BEFORE sed)" >/dev/null
grep -E '^127\.[[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+' /etc/hosts
sed -i -e '1,1 {/^127\.[[:digit:]]\+\.[[:digit:]]\+\.[[:digit:]]\+\t{{ node.fqdn }}\t{{ node.name }}/ d}' /etc/hosts
echo "State of /etc/hosts loopback assignments (AFTER sed)" >/dev/null
grep -E '^127\.[[:digit:]]+\.[[:digit:]]+\.[[:digit:]]+' /etc/hosts

# distribute SSH keys
cat /home/vagrant/.ssh/{{ ssh_key_name }}.pub >> /home/vagrant/.ssh/authorized_keys
[ ! -e "/root/.ssh" ] && mkdir /root/.ssh
chmod 600 /home/vagrant/.ssh/{{ ssh_key_name }}
cp /home/vagrant/.ssh/{{ ssh_key_name }}* /root/.ssh/
ln -s /root/.ssh/{{ ssh_key_name }} /root/.ssh/id_rsa
ln -s /root/.ssh/{{ ssh_key_name }}.pub /root/.ssh/id_rsa.pub
cat /root/.ssh/{{ ssh_key_name }}.pub >> /root/.ssh/authorized_keys

# disable host checking when SSHing within the cluster
cat >> /root/.ssh/config << 'EOF'
Host *
   StrictHostKeyChecking no
EOF

# set hostname
{% if fqdn %}
hostnamectl set-hostname {{ node.fqdn }}
{% else %}
hostnamectl set-hostname {{ node.name }}
{% endif %}

{% if version in ['octopus', 'ses7', 'pacific'] %}
# persist the journal
sed -i -e 's/#Storage=auto/Storage=persistent/' /etc/systemd/journald.conf
systemctl restart systemd-journald
{% endif %}{# version in ['octopus', 'ses7', 'pacific'] #}

# if --ssd option was given, set rotational flag on first additional disk
{% if ssd %}
if [ -f /sys/block/vdb/queue/rotational ] ; then
    echo "0" > /sys/block/vdb/queue/rotational
fi
{% endif %}{# ssd #}

{% if package_manager == 'zypper' %}
{% include "zypper.j2" ignore missing %}
{% elif package_manager == 'apt' %}
{% include "apt.j2" ignore missing %}
{% elif package_manager == 'yum' %}
{% include "yum.j2" ignore missing %}
{% elif package_manager == 'dnf' %}
{% include "dnf.j2" ignore missing %}
{% endif %}{# package_manager == 'zypper' #}

{% if not provision %}
set +x
echo "Stopping the deployment now because the --no-provision option was given"
exit 0
{% endif %}{# not provision #}

# include helper scripts
{% include "helper_scripts.j2" %}

# sync clocks if needed
{% include "sync_clocks.j2" %}

# apparmor
{% if apparmor %}
aa-status || true
{% else %}
aa-teardown
echo "Possibly changing GRUB configuration to disable apparmor persistently"
GRUB_APPARMOR_BEFORE="$(grep ^GRUB_CMDLINE_LINUX_DEFAULT /etc/default/grub)"
sed -i -e 's/^GRUB_CMDLINE_LINUX_DEFAULT=""$/GRUB_CMDLINE_LINUX_DEFAULT="apparmor=0"/' /etc/default/grub
GRUB_APPARMOR_AFTER="$(grep ^GRUB_CMDLINE_LINUX_DEFAULT /etc/default/grub)"
if [ "$GRUB_APPARMOR_AFTER" ] && [ "$GRUB_APPARMOR_BEFORE" != "$GRUB_APPARMOR_AFTER" ] ; then
    grub2-mkconfig -o /boot/grub2/grub.cfg
else
    echo "GRUB configuration did not change; not persisting it"
fi
aa-status || true
{% endif %}{# apparmor #}

{% if os != 'sles-12-sp3' %}
source /etc/os-release
if [[ "$ID_LIKE" =~ "suse" ]] ; then
    # enable coredump capturing
    sysctl -w kernel.core_pattern="|/usr/lib/systemd/systemd-coredump %P %u %g s %t %c %e"
    sysctl kernel.core_pattern
    cat <<EOF >>/etc/systemd/coredump.conf
ProcessSizeMax={{ ram }}G
ExternalSizeMax={{ ram }}G
JournalSizeMax={{ ram }}G
EOF
    cat /etc/systemd/coredump.conf
    systemctl enable systemd-coredump.socket
    systemctl start systemd-coredump.socket
fi
{% endif %}{# os != 'sles-12-sp3' #}

# deployment state machine

# DeepSea
{% if deploy_salt and deployment_tool == "deepsea" %}
{% include "salt/provision.sh.j2" %}
{% if node == master %}
{% include "salt/deepsea/deepsea_deployment.sh.j2" %}
{% include "salt/qa_test.j2" %}
{% endif %}{# node == master #}

# SUMA
{% elif suma and node == suma %}
{% include "salt/provision.sh.j2" %}
{% include "salt/suma/suma_deployment.sh.j2" %}

# ceph-salt (Day 1) + cephadm (Day 2)
{% elif deploy_salt and deployment_tool == "cephadm" %}
{% include "salt/provision.sh.j2" %}
{% if node == master %}
{% include "salt/ceph-salt/deployment_day_1.sh.j2" %}
{% include "cephadm/deployment_day_2.sh.j2" %}
{% include "salt/qa_test.j2" %}
{% endif %}{# node == master #}

# caasp4
{% elif version == 'caasp4' %}
{% include "caasp/provision.sh.j2" %}

# make check
{% elif version == 'makecheck' %}
{% include "makecheck/provision.sh.j2" %}

# upstream Ceph on Ubuntu
{% elif os.startswith('ubuntu') %}
{% include "ubuntu/provision.sh.j2" %}

{% endif %}{# end of deploy state machine #}

# inform user if reboot is needed (should be the very last thing the provisioner does)
zypper ps -s || true
