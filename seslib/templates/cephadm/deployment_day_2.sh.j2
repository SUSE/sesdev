{% if prometheus_image_path %}
ceph config set mgr mgr/cephadm/container_image_prometheus {{prometheus_image_path}}
{% endif %}
{% if grafana_image_path %}
ceph config set mgr mgr/cephadm/container_image_grafana {{grafana_image_path}}
{% endif %}
{% if alertmanager_image_path %}
ceph config set mgr mgr/cephadm/container_image_alertmanager {{alertmanager_image_path}}
{% endif %}
{% if node_exporter_image_path %}
ceph config set mgr mgr/cephadm/container_image_node_exporter {{node_exporter_image_path}}
{% endif %}
{% if keepalived_image_path %}
ceph config set mgr mgr/cephadm/container_image_keepalived {{keepalived_image_path}}
{% endif %}
{% if haproxy_image_path %}
ceph config set mgr mgr/cephadm/container_image_haproxy {{haproxy_image_path}}
{% endif %}
{% if snmp_gateway_image_path %}
ceph config set mgr mgr/cephadm/container_image_snmp_gateway {{snmp_gateway_image_path}}
{% endif %}

{% set service_spec_core = "/root/service_spec_core.yml" %}
rm -f {{ service_spec_core }}
touch {{ service_spec_core }}

{% set ceph_orch_apply_needed = False %}

{% if mon_nodes > 1 %}
{% set ceph_orch_apply_needed = True %}
cat >> {{ service_spec_core }} << 'EOF'
---
service_type: mon
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('mon') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('mon') #}
{% endfor %}
EOF
{% endif %}{# mon_nodes > 1 #}

{% if mgr_nodes > 1 %}
{% set ceph_orch_apply_needed = True %}
cat >> {{ service_spec_core }} << 'EOF'
---
service_type: mgr
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('mgr') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('mgr') #}
{% endfor %}
EOF
{% endif %}{# mgr_nodes > 1 #}

{% if storage_nodes > 0 %}
{% set ceph_orch_apply_needed = True %}
cat >> {{ service_spec_core }} << 'EOF'
---
service_type: osd
service_id: sesdev_osd_deployment
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('storage') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('storage') #}
{% endfor %}
data_devices:
    all: true
{% if encrypted_osds %}
encrypted: true
{% endif %}
{% if filestore_osds %}
objectstore: filestore
{% endif %}
EOF
{% endif %}{# storage_nodes > 0 #}

{% if ceph_orch_apply_needed %}
cat {{ service_spec_core }}
ceph orch apply -i {{ service_spec_core }} --dry-run || true
ceph orch apply -i {{ service_spec_core }}
{% endif %}{# ceph_orch_apply_needed #}

{% if total_osds > 0 %}

# Wait for OSDs to appear
function number_of_osds_in_ceph_osd_tree {
    ceph osd tree -f json-pretty | jq '[.nodes[] | select(.type == "osd")] | length'
}

EXPECTED_NUMBER_OF_OSDS="{{ total_osds }}"

set +x
timeout_seconds="$(({{ reasonable_timeout_in_seconds }} * 2))"
remaining_seconds="$timeout_seconds"
interval_seconds="30"
while true ; do
    ACTUAL_NUMBER_OF_OSDS="$(number_of_osds_in_ceph_osd_tree)"
    echo "OSDs in cluster (actual/expected): $ACTUAL_NUMBER_OF_OSDS/$EXPECTED_NUMBER_OF_OSDS (${remaining_seconds} seconds to timeout)"
    remaining_seconds="$(( remaining_seconds - interval_seconds ))"
    [ "$ACTUAL_NUMBER_OF_OSDS" = "$EXPECTED_NUMBER_OF_OSDS" ] && break
    if [ "$remaining_seconds" -le "0" ] ; then
        set -x
        ceph status
        set +x
        echo "It seems unlikely that this cluster will ever reach the expected number of OSDs. Bailing out!"
        ceph health detail
        false
    fi  
    sleep "$interval_seconds"
done
set -x

{% endif %}{# if total_osds > 0 #}

{% if storage_nodes > 0 %}

{% if nfs_nodes > 0 %}

NFS_NODES_COMMA_SEPARATED_LIST=""
{% for node in nodes %}
{% if node.has_role('nfs') %}
{% if fqdn %}
NFS_NODES_COMMA_SEPARATED_LIST+="{{ node.fqdn }},"
{% else %}
NFS_NODES_COMMA_SEPARATED_LIST+="{{ node.name }},"
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('nfs') #}
{% endfor %}
NFS_NODES_COMMA_SEPARATED_LIST="${NFS_NODES_COMMA_SEPARATED_LIST%,*}"

{% endif %}{# nfs_nodes > 0 #}

{% if mds_nodes > 0 %}

{% set service_spec_mds = "/root/service_spec_mds.yml" %}
{% set fs_name = "sesdev_fs" %}
{% set fs_pool_prefix = "cephfs." %}
{% set fs_meta_pool = fs_pool_prefix + fs_name + ".meta" %}
{% set fs_data_pool = fs_pool_prefix + fs_name + ".data" %}

ceph osd pool create {{ fs_meta_pool }}
ceph osd pool create {{ fs_data_pool }}

cat >> {{ service_spec_mds }} << 'EOF'
---
service_type: mds
service_id: {{ fs_name }}
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('mds') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('mds') #}
{% endfor %}
EOF
ceph orch apply -i {{ service_spec_mds }}

# wait for the MDSs to appear

function number_of_foos_ceph_orch_ls {
    local service_type="$1"
    local ceph_orch_ls
    local running
    ceph_orch_ls="$(ceph orch ls --service-type "$service_type" --format json)"
    if echo "$ceph_orch_ls" | jq -r >/dev/null 2>&1 ; then
        running="$(echo "$ceph_orch_ls" | jq -r '.[] | .status.running')"
        echo "$running"
    else
        echo "0"
    fi
}

EXPECTED_NUMBER_OF_MDSS="{{ mds_nodes }}"

set +x
timeout_seconds="{{ reasonable_timeout_in_seconds }}"
remaining_seconds="$timeout_seconds"
interval_seconds="30"
while true ; do
    ACTUAL_NUMBER_OF_MDSS="$(number_of_foos_ceph_orch_ls mds)"
    echo "MDSs in cluster (actual/expected): $ACTUAL_NUMBER_OF_MDSS/$EXPECTED_NUMBER_OF_MDSS (${remaining_seconds} seconds to timeout)"
    remaining_seconds="$(( remaining_seconds - interval_seconds ))"
    [ "$ACTUAL_NUMBER_OF_MDSS" = "$EXPECTED_NUMBER_OF_MDSS" ] && break
    if [ "$remaining_seconds" -le "0" ] ; then
        set -x
        ceph status
        set +x
        echo "It seems unlikely that this cluster will ever reach the expected number of MDSs. Bailing out!"
        ceph health detail
        false
    fi  
    sleep "$interval_seconds"
done
set -x

ceph fs new {{ fs_name }} {{ fs_meta_pool }} {{ fs_data_pool }}
sleep 10
ceph fs status
sleep 5
ceph fs status --format json-pretty

{% if nfs_nodes > 0 %}
{% set nfs_cluster = "sesdev_nfs" %}
{% if version in ['octopus', 'ses7'] %}
ceph nfs cluster create cephfs {{ nfs_cluster }} "$NFS_NODES_COMMA_SEPARATED_LIST"
{% else %}
ceph nfs cluster create {{ nfs_cluster }} "$NFS_NODES_COMMA_SEPARATED_LIST"
{% endif %}{# if version in ['octopus', 'ses7'] #}
sleep 10
ceph nfs cluster ls
{% endif %}{# nfs_nodes > 0 #}

{% endif %}{# mds_nodes > 0 #}

{% set deploy_monitoring = False %}
{% if prometheus_nodes > 0 %}
{% set deploy_monitoring = True %}
# manually enabling prometheus module will no longer be required once
# https://tracker.ceph.com/issues/46606 is fixed
ceph mgr module enable prometheus
{% endif %}{# prometheus_nodes > 0 #}
{% if grafana_nodes > 0 or alertmanager_nodes > 0 %}
{% set deploy_monitoring = True %}
{% endif %}{# grafana_nodes > 0 or alertmanager_nodes > 0 #}

{% if rgw_nodes > 0 or igw_nodes > 0 or deploy_monitoring %}

{% set service_spec_gw = "/root/service_spec_gw.yml" %}
rm -f {{ service_spec_gw }}
touch {{ service_spec_gw }}

{% if rgw_nodes > 0 %}
radosgw-admin realm create --rgw-realm=default --default
radosgw-admin zonegroup create --rgw-zonegroup=default --master --default
radosgw-admin zone create --rgw-zonegroup=default --rgw-zone=default --master --default
sleep 1  # might fail if started immediately after zone create
radosgw-admin period update --rgw-realm=default --commit
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: rgw
service_id: default.default
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('rgw') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('rgw') #}
{% endfor %}
EOF
{% endif %}{# rgw_nodes > 0 #}

# pre-create pool for IGW if needed
{% if igw_nodes > 0 %}
EXPONENT="$(({{ storage_nodes }} + 2))"
ceph osd pool create rbd "$((2 ** EXPONENT))"
{% endif %}{# igw_nodes > 0 #}

{% if igw_nodes > 0 %}
ceph osd pool application enable rbd rbd --yes-i-really-mean-it
{% set trusted_ip_list = [] %}
{% for node in nodes %}
{% if node.has_role('mgr') or node.has_role('igw') %}
{% if trusted_ip_list.append(node.public_address) %}{% endif %}
{% endif %}
{% endfor %}
cat >> {{ service_spec_gw }} << EOF
---
service_type: iscsi
service_id: iscsi_service
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('igw') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('igw') #}
{% endfor %}
spec:
    pool: rbd
    trusted_ip_list: {{ trusted_ip_list | join(',') }}
    api_port: 5000
    api_user: admin1
    api_password: admin2
    api_secure: False
EOF
{% endif %}{# igw_nodes > 0 #}

{% if prometheus_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: prometheus
service_id: prometheus
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('prometheus') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('prometheus') #}
{% endfor %}
EOF
{% endif %}{# prometheus_nodes > 0 #}

{% if grafana_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: grafana
service_id: grafana
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('grafana') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('grafana') #}
{% endfor %}
EOF
{% endif %}{# grafana_nodes > 0 #}

{% if alertmanager_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: alertmanager
service_id: alertmanager
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('alertmanager') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('alertmanager') #}
{% endfor %}
EOF
{% endif %}{# alertmanager_nodes > 0 #}

{% if node_exporter_nodes > 0 %}
cat >> {{ service_spec_gw }} << 'EOF'
---
service_type: node-exporter
service_id: node-exporter
placement:
    hosts:
{% for node in nodes %}
{% if node.has_role('node-exporter') %}
{% if fqdn %}
        - '{{ node.fqdn }}'
{% else %}
        - '{{ node.name }}'
{% endif %}{# fqdn #}
{% endif %}{# node.has_role('node-exporter') #}
{% endfor %}
EOF
{% endif %}{# node_exporter_nodes > 0 #}

cat {{ service_spec_gw }}
ceph orch apply -i {{ service_spec_gw }}

{% endif %}{# rgw_nodes > 0 or igw_nodes > 0 or deploy_monitoring #}

{% endif %}{# storage_nodes > 0 #}

ceph config set mon auth_allow_insecure_global_id_reclaim false || true
