
zypper --non-interactive install --type pattern SUSE-CaaSP-Management

{% if node.name == 'master' %}

function wait_for_master_ready {
    set +ex
    echo "Waiting for master to be ready"
    timeout_seconds="900"
    remaining_seconds="$timeout_seconds"
    interval_seconds="10"
    while true ; do
        set -x
        ACTUAL_NUMBER_OF_MASTERS="$(kubectl get nodes 2>/dev/null | egrep -c "master\s+Ready")"
        set +x
        echo "masters in cluster (actual/expected): $ACTUAL_NUMBER_OF_MASTERS/1 (${remaining_seconds} seconds to timeout)"
        remaining_seconds="$(( remaining_seconds - interval_seconds ))"
        [ "$ACTUAL_NUMBER_OF_MASTERS" = "1" ] && break
        if [ "$remaining_seconds" -le "0" ] ; then
            echo "It seems unlikely that a master will ever appear. Bailing out!"
            set -e
            false
        fi  
        sleep "$interval_seconds"
    done
    set -ex
}

function wait_for_workers_ready {
    set +ex
    echo "Waiting for {{ worker_nodes }} workers to be ready"
    timeout_seconds="900"
    remaining_seconds="$timeout_seconds"
    interval_seconds="10"
    while true ; do
        set -x
        ACTUAL_NUMBER_OF_WORKERS="$(kubectl get nodes 2>/dev/null | egrep -c "worker[0-9]+\s+Ready")"
        set +x
        echo "workers in cluster (actual/expected): $ACTUAL_NUMBER_OF_WORKERS/{{ worker_nodes }} (${remaining_seconds} seconds to timeout)"
        remaining_seconds="$(( remaining_seconds - interval_seconds ))"
        [ "$ACTUAL_NUMBER_OF_WORKERS" = "{{ worker_nodes }}" ] && break
        if [ "$remaining_seconds" -le "0" ] ; then
            echo "It seems unlikely that {{ worker_nodes }} workers will ever appear. Bailing out!"
            set -e
            false
        fi  
        sleep "$interval_seconds"
    done
    set -ex
}

zypper --non-interactive install skuba

touch /tmp/ready

while : ; do
  PROVISIONED_NODES=`ls -l /tmp/ready-* 2>/dev/null | wc -l`
  echo "waiting for nodes (${PROVISIONED_NODES}/{{ nodes|length }})";
  [[ "${PROVISIONED_NODES}" != "{{ nodes|length }}" ]] || break
  sleep 2;
{% for node in nodes %}
  scp -o StrictHostKeyChecking=no {{ node.name }}:/tmp/ready /tmp/ready-{{ node.name }};
{% endfor %}
done

SKUBA_VERBOSITY=${SKUBA_VERBOSITY:-1}

eval $(ssh-agent -s)
ssh-add ~/.ssh/id_rsa

mkdir -p ~/cluster
cd ~/cluster

{% if node_manager.get_one_by_role('loadbalancer') %}
skuba -v ${SKUBA_VERBOSITY} cluster init --control-plane {{ node_manager.get_one_by_role('loadbalancer').name }} caasp4-cluster
{% else %}
skuba -v ${SKUBA_VERBOSITY} cluster init --control-plane {{ node_manager.get_one_by_role('master').name }} caasp4-cluster
{% endif %}

chmod g+rx caasp4-cluster
cd caasp4-cluster
skuba -v ${SKUBA_VERBOSITY} node bootstrap --user sles --sudo --target {{ node.name }} {{ node.name }}
skuba -v ${SKUBA_VERBOSITY} cluster status
mkdir -p ~/.kube
ln -sf /root/cluster/caasp4-cluster/admin.conf ~/.kube/config
chmod g+r /root/cluster/caasp4-cluster/admin.conf
kubectl get nodes -o wide

# adding masters
{% for _node in node_manager.get_by_role('master') %}
{% if _node != node %}
skuba -v ${SKUBA_VERBOSITY} node join --role master --user sles --sudo --target {{ _node.name }} {{ _node.name }}
{% endif %}
{% endfor %}
skuba -v ${SKUBA_VERBOSITY} cluster status
kubectl get nodes -o wide

# adding workers
{% for _node in node_manager.get_by_role('worker') %}
skuba -v ${SKUBA_VERBOSITY} node join --role worker --user sles --sudo --target {{ _node.name }} {{ _node.name }}
{% endfor %}

wait_for_master_ready
wait_for_workers_ready

skuba -v ${SKUBA_VERBOSITY} cluster status
kubectl get nodes -o wide

{% if nodes|length == 1 %}

kubectl taint nodes --all node-role.kubernetes.io/master-

{% endif %}

# setting up helm
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account=tiller --wait

{% if nfs_nodes > 0 %}
# adding nfs storage class
helm install --name=nfs-client --set nfs.server=nfs1 --set nfs.path=/nfs --set storageClass.defaultClass=true stable/nfs-client-provisioner
{% endif %}

# Installing Kubernetes Dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc2/aio/deploy/recommended.yaml
kubectl patch svc kubernetes-dashboard --type='json' -p '[{"op":"replace","path":"/spec/type","value":"NodePort"}]' -n kubernetes-dashboard

cat >/tmp/dashboard-admin.yaml <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
EOF

kubectl apply -f /tmp/dashboard-admin.yaml

cat >/tmp/admin-user-crb.yaml <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: admin-user
    namespace: kube-system
EOF

kubectl apply -f /tmp/admin-user-crb.yaml

rm -f /tmp/dashboard-admin.yaml /tmp/admin-user-crb.yaml 2>/dev/null

ST=$(kubectl -n kube-system get serviceaccounts admin-user -o jsonpath="{.secrets[0].name}")
SECRET=$(kubectl -n kube-system get secret ${ST} -o jsonpath="{.data.token}"|base64 -d)
export NODE_PORT=$(kubectl get -o jsonpath="{.spec.ports[0].nodePort}" services kubernetes-dashboard -n kubernetes-dashboard)
export NODE_IP=$(kubectl get nodes -o jsonpath="{.items[0].status.addresses[0].address}" -n kubernetes-dashboard)

echo "    token: $SECRET" >> ~/.kube/config
echo "Access your dashboard at: https://$NODE_IP:$NODE_PORT/"
echo "Your login token is: ${SECRET}"
echo "Or use ~/.kube/config to authenticate with kubeconfig"

# setting up MetalLB
MLBCONFIG=/tmp/metallb.yaml

kubectl create namespace metallb-system

cat > ${MLBCONFIG} <<EOF
configInline:
  address-pools:
  - name: default
    protocol: layer2
    addresses:
    - 192.168.121.240-192.168.121.250
EOF

# checking if api available for metallb, could take sometime for single node cluster
kubectl --namespace=kube-system wait --for=condition=Available --timeout=1m apiservices/v1beta1.metrics.k8s.io
kubectl get apiservice

helm install --namespace metallb-system --name metallb stable/metallb -f ${MLBCONFIG}
rm ${MLBCONFIG}

{% if caasp_deploy_ses %}

# deploy and configure rook + Ceph

# add special lable to allow Rook deploy on all nodes
kubectl label nodes --all node-role.rook-ceph/cluster=any

zypper --non-interactive install rook-k8s-yaml
cd /usr/share/k8s-yaml/rook/ceph/

{% if node_manager.get_by_role('worker') | length < 3 %}

sed -i 's/allowMultiplePerNode: false/allowMultiplePerNode: true/' /usr/share/k8s-yaml/rook/ceph/cluster.yaml

{% endif %} {# node_manager.get_by_role('worker') | length < 3 #}

{% if nodes|length == 1 %}

sed -i 's/count: .*/count: 1/' /usr/share/k8s-yaml/rook/ceph/cluster.yaml

{% endif %}

kubectl apply -f common.yaml -f operator.yaml
sleep 15
kubectl apply -f cluster.yaml -f toolbox.yaml
echo "Verify the installation by running 'kubectl get pods -n rook-ceph'"

{% endif %} {# caasp_deploy_ses #}

{% else %}

touch /tmp/ready

{% endif %}
